{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow api 사용 -> version 차이 맞춰야 하고 중간 과정 이해 필요함... (해볼예정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Unnamed: 0  userID  users_mean_rating  user_reviewcount  locationId  \\\n0           0       0                5.0                 1     3656150   \n1           1       1                5.0                 1      301253   \n2           2       2                5.0                 1      306139   \n3           3       3                4.2                10     4071058   \n4           4       3                4.2                10    11959139   \n\n   rating      placeType  createdDate  islocal  photonum  \n0       5  ACCOMMODATION     20140121        0         0  \n1       5  ACCOMMODATION     20161005        0         2  \n2       5  ACCOMMODATION     20140324        0         0  \n3       5         EATERY     20161113        1         0  \n4       5         EATERY     20170405        1         1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>userID</th>\n      <th>users_mean_rating</th>\n      <th>user_reviewcount</th>\n      <th>locationId</th>\n      <th>rating</th>\n      <th>placeType</th>\n      <th>createdDate</th>\n      <th>islocal</th>\n      <th>photonum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>3656150</td>\n      <td>5</td>\n      <td>ACCOMMODATION</td>\n      <td>20140121</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>301253</td>\n      <td>5</td>\n      <td>ACCOMMODATION</td>\n      <td>20161005</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>306139</td>\n      <td>5</td>\n      <td>ACCOMMODATION</td>\n      <td>20140324</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>4.2</td>\n      <td>10</td>\n      <td>4071058</td>\n      <td>5</td>\n      <td>EATERY</td>\n      <td>20161113</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3</td>\n      <td>4.2</td>\n      <td>10</td>\n      <td>11959139</td>\n      <td>5</td>\n      <td>EATERY</td>\n      <td>20170405</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "new_df = pd.read_csv(os.path.join(\"..\",\"..\",\"data\",\"TA_User_Reviws_Korea_all_v2_new_df.csv\"))\n",
    "new_df = new_df.drop(['location_name'], axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. wide 부분에 대한 features 선택 : 사용하길 원하는 sparse base column 과 crossed column 들을 선택한다  \n",
    "2. deep 부분에 대한 features 선택 : 연속된 열, 각 분류 열의 embedding dimension, 그리고 hidden layer 크기를 선택한다  \n",
    "3. 이들을 Wide & Deep 모델에 적용한다(DNNLinearCombinedClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Feature Columns 정의  \n",
    "사용할 base categorical feature column 과 continuous feature column 들을 정의하자. 이들 base column 들은 모델의 wide 부분과 deep 부분에 모두 사용될 building block 들이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras.layers' has no attribute 'sparse_column_with_keys'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-07d339cb747b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Categorical base columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_column_with_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"placeType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ACCOMMODATION\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"EATERY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Continuous base columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.layers' has no attribute 'sparse_column_with_keys'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Categorical base columns.\n",
    "placeType = tf.keras.layers.sparse_column_with_keys(column_name=\"placeType\", keys=[\"ACCOMMODATION\", \"EATERY\"])\n",
    "\n",
    "# Continuous base columns.\n",
    "userID = tf.contrib.layers.real_valued_column(\"userID\")\n",
    "users_mean_rating = tf.contrib.layers.real_valued_column(\"users_mean_rating\")\n",
    "user_reviewcount = tf.contrib.layers.real_valued_column(\"user_reviewcount\")\n",
    "locationId = tf.contrib.layers.real_valued_column(\"locationId\")\n",
    "createdDate = tf.contrib.layers.real_valued_column(\"createdDate\")\n",
    "photonum = tf.contrib.layers.real_valued_column(\"photonum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wide Model: Linear Model with Crossed Feature Columns  \n",
    "wide 모델은 sparse 하고 crosseed feature column 들의 다양한 집합의 선형 모델이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns = [\n",
    "  placeType, \n",
    "  tf.contrib.layers.crossed_column([education, occupation], hash_bucket_size=int(1e4)),\n",
    "  tf.contrib.layers.crossed_column([native_country, occupation], hash_bucket_size=int(1e4)),\n",
    "  tf.contrib.layers.crossed_column([age_buckets, race, occupation], hash_bucket_size=int(1e6))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crossed feature column 들의 Wide 모델은 feature들 간의 sparse interactions(상호작용들)을 효율적으로 저장할 수 있다. 그럼에도, crossed feature columns 의 한가지 제한되는 점은 학습된 데이터에 나타나지 않는 feature 조합들을 생성해내지 못한다는 것이다. 이를 수정하기 위해 embeddings 를 포함한 deep 모델을 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Model: Neural Network with Embeddings\n",
    "이전 그림에서 보이는 것과 같이 deep 모델은 feed-forward 신경망이다. 각각의 sparse, high-dimensional(고차원의) categorical feature 들은 종종 embedding 벡터라 불리는 low-dimensional(저차원의), dense real-valued 벡터로 변환된다. 이 low-dimensional(저차원의) dense embedding 벡터들은 continuouse feature 들로 연계되어 변환되고, 계속해서 다음 신경망의 hidden layers 로 연결된다. embedding value 들은 무작위로 초기화되고, 학습 loss(손실)을 최소화하는 다른 모든 모델 파라미터들과 함께 학습된다.    \n",
    "  \n",
    "embedding_column 을 이용하여 categorical column 들을 위한 embeddings 를 설정할 것이다. 그리고 이들을 continuous columns 과 연관지을 것이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_columns = [\n",
    "  tf.contrib.layers.embedding_column(placeType, dimension=8),\n",
    "  userID, users_mean_rating, user_reviewcount, locationId, createdDate, photonum]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense embeddings 를 통해, deep 모델은 학습된 데이터에서 이전에 보여지지 못한 feature 쌍들에 대한 생성을 더 잘 할 수 있고 예측할 수 있다. 그러나 두 feature column 간의 기본 interaction matrix(상호작용 메트릭스)가 sparse 하고 high-rank 일 때, feature column 들에 대한 low-dimensional representation 들의 효율적인 학습은 어렵다. 이러한 경우, 대부분의 feature 쌍 들간 interaction(상호작용)은 몇몇을 제외하는 0(zero) 이 되어야 한다. 하지만 dense embeddings 는 모든 feature 쌍들에 대해 none-zero(0 이 아닌) 예측을 이끌어 냄에 따라, 지나치게 일반화할 수 있다. 반면, crossed features 선형모델은 이러한 \"exception rules\"(예외들)을 더 적은 모델 파라미터들을 이용해 효율적으로 저장할 수 있다.\n",
    "이제 어떻게 wide 모델과 deep모델을 함께 학습시키고 그들 각각의 강점과 약점을 보완해 가는지 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep 모델을 하나로 조합하기\n",
    "wide 모델과 deep 모델들은 예측으로써 그들의 마지막 출력 로그 나머지(final output log odd) 의 합에 의해 조합된다. 그 다음 로지스틱 loss 함수의 예측에 전달된다. 모든 그래프 정의와 변수 할당량들은 이미 내부에서 다뤄졌기 때문에, DNNLinearCombinedClassifier 를 만들어 볼 필요가 있다.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "model_dir = tempfile.mkdtemp()\n",
    "m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습하기와 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "# Define the column names for the data sets.\n",
    "COLUMNS = [\"placeType\" ,\"userID\", \"users_mean_rating\", \"user_reviewcount\", \"locationId\",\n",
    "                      \"createdDate\", \"photonum\"]\n",
    "LABEL_COLUMN = 'rating'\n",
    "CATEGORICAL_COLUMNS = [\"placeType\"]\n",
    "CONTINUOUS_COLUMNS = [\"userID\", \"users_mean_rating\", \"user_reviewcount\", \"locationId\",\n",
    "                      \"createdDate\", \"photonum\"]\n",
    "\n",
    "\n",
    "def input_fn(df):\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values)\n",
    "                     for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label\n",
    "\n",
    "def train_input_fn():\n",
    "  return input_fn(df_train)\n",
    "\n",
    "def eval_input_fn():\n",
    "  return input_fn(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 읽고 난 후, 모델을 학습하고 평가할 수 있다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(input_fn=train_input_fn, steps=200)\n",
    "results = m.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "for key in sorted(results):\n",
    "  print \"%s: %s\" % (key, results[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}